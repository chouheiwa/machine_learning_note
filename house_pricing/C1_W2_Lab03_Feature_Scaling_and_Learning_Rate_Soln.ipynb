{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_multi import  load_house_data, run_gradient_descent\n",
    "from lab_utils_multi import  norm_plot, plt_equal_scale, plot_cost_i_w\n",
    "from lab_utils_common import dlc\n",
    "np.set_printoptions(precision=2)\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 准备数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "X_train, y_train = load_house_data()\n",
    "X_features = ['size(sqft)','bedrooms','floors','age']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 观察数据集特征"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for index in range(len(ax)):\n",
    "    ax[index].scatter(X_train[:,index],y_train)\n",
    "    ax[index].set_xlabel(X_features[index])\n",
    "ax[0].set_ylabel(\"Price (1000's)\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 预测函数\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x})=\\mathbf{w}\\cdot\\mathbf{x}+b$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_value(x: np.ndarray, w: np.ndarray, b):\n",
    "    return np.dot(x, w) + b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 计算损失函数\n",
    "$$J(\\mathbf{w},b)=\\frac{1}{2m}\\sum_{i=0}^{n-1}(f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) - y^{(i)})^2$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_cost(x: np.ndarray, y: np.ndarray, w: np.ndarray, b):\n",
    "    m = x.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        cost += (predict_value(x[i], w, b) - y[i]) ** 2\n",
    "    return cost / (2 * m)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 计算梯度\n",
    "$$\\frac{\\partial}{\\partial w_j}J(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=0}^{n-1}(f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) - y^{(i)})x_j^{(i)} $$\n",
    "$$\\frac{\\partial}{\\partial b}J(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=0}^{n-1}(f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) - y^{(i)}) $$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_gradient(x: np.ndarray, y: np.ndarray, w: np.ndarray, b):\n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        err = predict_value(x[i], w, b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * x[i, j]\n",
    "        dj_db += err\n",
    "\n",
    "    return dj_dw / m, dj_db / m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 梯度下降\n",
    "$$\n",
    "w:= w - \\frac{\\partial}{\\partial w_j}J(\\mathbf{w}, b) \\\\\n",
    "b:= b - \\frac{\\partial}{\\partial b}J(\\mathbf{w}, b)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gradient_descend(x, y, w_in, b_in, alpha, num_iterations, cost_function, gradient_function):\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    J_history = []\n",
    "    for i in range(num_iterations):\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "        cost = cost_function(x, y, w, b)\n",
    "        if i < 100000:\n",
    "            J_history.append(cost)\n",
    "        if i % math.ceil(num_iterations / 10) == 0:\n",
    "            print(f'循环执行了{i}次， 当前预测函数误差为:{cost:0.2f}')\n",
    "    return w, b, J_history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 训练模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m, n = X_train.shape\n",
    "alpha = 9.1e-7\n",
    "num_iterations = 10000\n",
    "\n",
    "w, b, J_history = gradient_descend(X_train, y_train, np.zeros((n,)), 0, alpha, num_iterations, compute_cost, compute_gradient)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 绘制图形"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_history)\n",
    "ax2.plot(100 + np.arange(len(J_history[100:])), J_history[100:])\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost')\n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 特征缩放 - z-score 标准化(zero-mean normalization)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    计算矩阵X的z-score标准化(以列为特征)\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))     : 输入数据集, m个训练样本, n个特征向量\n",
    "\n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    X_norm = (X - mu) / sigma\n",
    "    return X_norm, mu, sigma"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对标准化进行展示"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_norm, mu, sigma = zscore_normalize_features(X_train)\n",
    "X_mean = X_train - mu\n",
    "\n",
    "_,ax=plt.subplots(1, 3, figsize=(12, 3))\n",
    "ax[0].scatter(X_train[:,0], X_train[:,3])\n",
    "ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[0].set_title(\"unnormalized\")\n",
    "ax[0].axis('equal')\n",
    "\n",
    "ax[1].scatter(X_mean[:,0], X_mean[:,3])\n",
    "ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[1].set_title(r\"X - $\\mu$\")\n",
    "ax[1].axis('equal')\n",
    "\n",
    "ax[2].scatter(X_norm[:,0], X_norm[:,3])\n",
    "ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[2].set_title(r\"Z-score normalized\")\n",
    "ax[2].axis('equal')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.suptitle(\"distribution of features before, during, after normalization\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}